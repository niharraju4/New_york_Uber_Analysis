# -*- coding: utf-8 -*-
"""Uber_Newyork_DataAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EcDOtmqP1l8kk50vzcaqe1I8M04GFcRw
"""

#LIBRARIES
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import chart_studio.plotly as py
import plotly.graph_objs as go
import plotly.express as px
import folium

from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

import os # to call files or use files and datasets from the local drive

os.listdir(r"N:\datasets\Datasets") #this list calls all the files from the os

"""# here R or r is used to read all the files even if there is error is in backslash or frontslash"""

df = pd.read_csv(r"N:\datasets\Datasets/uber-raw-data-janjune-15_sample.csv")

df #print the data frame

df.shape #printing the shape to check the data frame

"""# Data preprocessing ot data cleaning"""

df.duplicated

"""# df.duplicated() is typically used when you want to directly access the boolean Series indicating duplicates."""

df.duplicated() # when you add paranthesis it consideres itself into two dimensional true or flase types

"""# df.duplicated().sum() is used to count the total number of duplicate rows in the DataFrame, which can be useful for reporting or further processing"""

df.duplicated().sum()

"""# drop_duplicates,To drop all the duplicates and here inplace refers as to whatever places it is false it make true and rearrages the dataframe"""

df.drop_duplicates(inplace=True)

df1 = df.copy()

df1.drop_duplicates(inplace = True)

df1.shape # recheck again if the duplicates are removed and shape is

df1

"""# Here im checking what kind of data is availabe in each type of columns"""

df1.dtypes

"""# Here im checking if isnull is making meaning if there are any null values"""

df1.isnull().sum()

df1.isnull()

"""# Checking the date and time"""

df1['Pickup_date'][0]

"""# Also checking the type of the date and time is it a str or int or it should be in datetime format"""

type(df1['Pickup_date'][0])

"""# This is the way to convert a pandas to datetime pd.to_datetime(df['Pickup_date'])"""

pd.to_datetime(df1['Pickup_date'])

df1['Pickup_date'] = pd.to_datetime(df1['Pickup_date'])

df1['Pickup_date'].dtype

df1['Pickup_date'][0] #So now its got converted to a timestamp for the normal time

type(df1['Pickup_date'][0]) # now the pandas is converted it to the timestamp

df1.dtypes #here now if you see the pickupdate is converted from an object to timestamp

"""# Categrorical data can be either object or bool it can be taken as groups/ categroies. numerical data is either integer which is discrete, float which is a continuous

Integer has signed and unsigned where signed can be postieve and negative where as unsigned is postieve

# problem statement which month is the highest pickup of uber in NEW YORK
"""

df1

"""# dt.month is used to return months in the form of numbers"""

df1['Pickup_date'].dt.month

df1['month'] = df1['Pickup_date'].dt.month_name()# it prints all the months name

df1['month']

df1['month'].value_counts()

df1['month'].value_counts().plot(kind = 'bar')

"""# for different days to execute"""

df1['Pickup_date'].dt.day_name()

"""# for hourly"""

df1['Pickup_date'].dt.hour

"""# for every minute"""

df1['Pickup_date'].dt.minute

"""# printing the first 5 values"""

df1.head(5)

"""# printing the last five values"""

df1.tail(5)

df1['Day_of_Week'] = df1['Pickup_date'].dt.day_name()

"""# to do cross tabulation"""

df1['weekday'] = df1['Pickup_date'].dt.day_name()
pivot = pd.crosstab(index=df1['month'], columns=df1['weekday'])

pivot

pivot.plot(kind='bar', figsize=(8,6))

"""# Analying the rush hour in new york [Data Analysis]"""

pivot.plot(kind='line', figsize=(8,6))

df1['hour'] = df1['Pickup_date'].dt.hour

summary = df1.groupby(['weekday', 'hour'], as_index=False).size()

summary

plt.figure(figsize=(8, 6))
sns.pointplot(x="hour", y="size", hue="weekday", data=summary)
plt.show()

"""# Which base number has the most amount of active vechiles??"""

df1.columns

os.listdir(r"N:\datasets\Datasets") #this list calls all the files from the os

df_foil = pd.read_csv(r'N:\datasets\Datasets\Uber-Jan-Feb-FOIL.csv')

df_foil

df_foil.head()

init_notebook_mode(connected=True)

df_foil.columns

"""# BOXPLOT USING PLOTLY"""

px.box(x='dispatching_base_number', y='active_vehicles', data_frame=df_foil)

"""# Violin plot -> which means distance + box"""

px.violin(x='dispatching_base_number', y='active_vehicles', data_frame=df_foil)

os.listdir(r"N:\datasets\Datasets") #this list calls all the files from the os

"""# Now lets combine data into one big data or some new data file"""

files = os.listdir(r"N:\datasets\Datasets")[-9:-1]
files

files.remove('uber-raw-data-janjune-15.csv')
files

files.remove('uber-raw-data-janjune-15_sample.csv')
files

"""# Concating all these files together"""

final_df = pd.DataFrame()

path = r"N:\datasets\Datasets"

for file in files:
    current_df = pd.read_csv(path+'/'+file)
    final_df = pd.concat([current_df, final_df])

final_df.shape

"""Since we merged all the 6 .csv files lets check weather we have ny duplicate rows and do all
the data cleaning process
"""

final_df.duplicated().sum()

final_df.drop_duplicates(inplace = True)

final_df.shape

final_df

"""# At what locations of new york city we are getting rush"""

rush_uber = final_df.groupby(['Lat','Lon'], as_index=False).size()

"""# Folium is a map"""

basemap = folium.Map()
basemap

from folium.plugins import HeatMap

HeatMap(rush_uber).add_to(basemap)

basemap

"""# Examine Rush on hour and weekday( Performing pairwise analysis)"""

final_df.columns

final_df.head()

final_df.dtypes

final_df['Date/Time'][0] # here we will get 6 indexs because we have concat 6.csv files for 0 right so

final_df['Date/Time']=pd.to_datetime(final_df['Date/Time'], format="%m/%d/%Y %H:%M:%S")
final_df['Date/Time']

final_df['Date/Time'].dtype

"""# for day"""

final_df['day'] = final_df['Date/Time'].dt.day
final_df['day']

"""# for hour"""

final_df['hour'] = final_df['Date/Time'].dt.hour
final_df['hour']

final_df.head()

final_df.groupby(['day','hour']). size()

final_df.groupby(['day','hour'], as_index=False). size()

pivot1 = final_df.groupby(['day','hour']). size().unstack()
pivot1

pivot1.style.background_gradient()

"""# Lets create a def and put everything inside it."""

def get_pivot_table(df, col1, col2):
    pivot1 = final_df.groupby([col1, col2]).size().unstack()
    return pivot1.style.background_gradient()

get_pivot_table(final_df, "day", "hour")